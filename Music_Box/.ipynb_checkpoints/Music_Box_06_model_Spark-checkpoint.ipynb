{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Music Box Churn Prediction and Recommendation using Spark\n",
    "\n",
    "# Using Spark to train model\n",
    "\n",
    "# Goal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext('local')\n",
    "# spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.mllib.linalg import Vectors, DenseVector\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import LogisticRegression, DecisionTreeClassifier, RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer, VectorIndexer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv('data/model_final.csv', header=True, inferSchema=True).cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Select features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = df.columns\n",
    "selected_features.remove('uid')\n",
    "selected_features.remove('label')\n",
    "selected_features.remove('device_type')\n",
    "selected_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=selected_features, outputCol='features')\n",
    "data = assembler.transform(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train, test) = data.randomSplit([0.7, 0.3], seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define prediction function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(model, data):\n",
    "    # Predict data\n",
    "    predictions = model.transform(data)\n",
    "    \n",
    "    # Select example rows to display\n",
    "    predictions.select('probability', 'prediction', 'label', 'features').show(5)\n",
    "    res_data = predictions.select('probability', 'label').toPandas()\n",
    "    return res_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Predict \n",
    "\n",
    "# # Predict train data\n",
    "# predictions_train = model.trainsform(train)\n",
    "\n",
    "# # Select example rows to display\n",
    "# predictions_train.select('probability', 'prediction', 'label', 'features').show(5)\n",
    "# res_train = predictions_train.select('probability', 'label').toPandas()\n",
    "\n",
    "# # Predict test data\n",
    "# predictions_test = model.transform(test)\n",
    "\n",
    "# # Select example rows to display\n",
    "# predictions_test.select('probability', 'prediction', 'label', 'features').show(5)\n",
    "# res_test = predictions_test.select('probability', 'label').toPendas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define ploting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import auc, roc_curve, roc_auc_score\n",
    "\n",
    "def plot_roc_curve(y_train, p_train_pred, y_test, p_test_pred):\n",
    "    roc_auc_train = roc_auc_score(y_train, p_train_pred)\n",
    "    fpr_train, tpr_train, _ = roc_curve(y_train, p_train_pred)\n",
    "    \n",
    "    roc_auc_test = roc_auc_score(y_test, p_test_pred)\n",
    "    fpr_test, tpr_test, _ = roc_curve(y_test, p_test_pred)\n",
    "    \n",
    "    lw = 2\n",
    "    plt.figure()\n",
    "    plt.plot(fpr_train, tpr_train, color='green', linewidth=lw, label='ROC Train (AUC = %0.4f)' % roc_auc_train)\n",
    "    plt.plot(fpr_test, tpr_test, color='darkorange', linewidth=lw, label='ROC Test (AUC = %0.4f)' % roc_auc_test)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', linewidth=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver operating characteristic example')\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define predict and evaluate performance function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_evaluate_performance(model, train, test):\n",
    "    # Predict\n",
    "    pred_train = predictions(model, train)\n",
    "    pred_test = predictions(model, test)\n",
    "\n",
    "    y_train = pred_train['label']\n",
    "    p_train_pred = [v[1] for v in pred_train['probability']]\n",
    "\n",
    "    y_test = pred_test['label']\n",
    "    p_test_pred = [v[1] for v in pred_test['probability']]\n",
    "\n",
    "    # Evaluate\n",
    "    plot_roc_curve(y_train, p_train_pred, y_test, p_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(maxIter=3, regParam=0.01)\n",
    "lr_model = lr.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_evaluate_performance(lr_model, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict\n",
    "# lr_pred_train = predictions(lr_model, train)\n",
    "# lr_pred_test = predictions(lr_model, test)\n",
    "\n",
    "# y_train = lr_pred_train['label']\n",
    "# p_train_pred = [v[1] for v in lr_pred_train['probability']]\n",
    "\n",
    "# y_test = lr_pred_test['label']\n",
    "# p_test_pred = [v[1] for v in lr_pred_test['probability']]\n",
    "\n",
    "# # Evaluate\n",
    "# plot_roc_curve(y_train, p_train_pred, y_test, p_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(labelCol='label',\n",
    "                            featuresCol='features',\n",
    "                            numTrees=50,\n",
    "                            featureSubsetStrategy='auto',\n",
    "                            impurity='gini',\n",
    "                            maxDepth=12,\n",
    "                            minInstancesPerNode=10,\n",
    "                            maxBins=16)\n",
    "rf_model = rf.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict and evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_evaluate_performance(rf_model, train, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Predict\n",
    "# rf_pred_train = predictions(rf_model, train)\n",
    "# rf_pred_test = predictions(rf_model, test)\n",
    "\n",
    "# y_train = rf_pred_train['label']\n",
    "# p_train_pred = [v[1] for v in rf_pred_train['probability']]\n",
    "\n",
    "# y_test = pred_test['label']\n",
    "# p_test_pred = [v[1] for v in rf_pred_test['probability']]\n",
    "\n",
    "# # Evaluate\n",
    "# plot_roc_curve(y_train, p_train_pred, y_test, p_test_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
