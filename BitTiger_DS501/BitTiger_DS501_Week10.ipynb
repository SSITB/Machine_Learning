{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BitTiger DS501 Week10\n",
    "\n",
    "## Feedforward Neural Net (FNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras.callbacks as cb\n",
    "from keras.datasets import mnist\n",
    "from keras.layers.core import Activation, Dense, Dropout\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pre-processing\n",
    "\n",
    "### 分成 3 個 Groups:\n",
    "1. A: 沒有 pre-processing\n",
    "2. B: 用 normalization 把 dataset normalize 到 [0, 1] 之間\n",
    "\\begin{equation}x' = \\frac{x - x_{min}}{x_{max} - x_{min}}\\end{equation}\n",
    "3. C: 用 standardization 把 dataset 變成 mean=0, variance=1 的高斯分佈\n",
    "\\begin{equation}x_i' = \\frac{x_i - \\mu}{\\sigma}\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(group):\n",
    "    from sklearn import preprocessing\n",
    "    # Load MNIST dataset from Keras (https://keras.io/datasets/)\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    \n",
    "    # Transform labels to one-hot\n",
    "    y_train = np_utils.to_categorical(y_train, 10)\n",
    "    y_test = np_utils.to_categorical(y_test, 10)\n",
    "    \n",
    "    # Set features to numeric type\n",
    "    X_train = X_train.astype('float32')\n",
    "    X_test = X_test.astype('float32')\n",
    "    # Reshape from 28 x 28 to 1-D vector\n",
    "    X_train = np.reshape(X_train, (60000, 784))\n",
    "    X_test = np.reshape(X_test, (10000, 784))\n",
    "    \n",
    "    if group == 'A':\n",
    "        return X_train, X_test, y_train, y_test\n",
    "    elif group == 'B':\n",
    "        X_train /= 255\n",
    "        X_test /= 255\n",
    "    elif group == 'C':\n",
    "        X_train = preprocessing.scale(X_train)\n",
    "        X_test = preprocessing.scale(X_test)\n",
    "        \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group = 'B'\n",
    "X_train, X_test, y_train, y_test = Preprocessing(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 檢查一下 training data\n",
    "\n",
    "只看前 10 筆資料"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     X                      |  y  \n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print('{:^43}'.format('X'), '|', '{:^4}'.format('y'))\n",
    "print('='*50)\n",
    "for row in range(10):\n",
    "    print('{:.2f} {.2f} ... {:.2f} {:.2f} {:.2f}  ... {:.2f} {:.2f} '.format(\n",
    "            X_train[row][0], X_train[row][1],\n",
    "            X_train[row][156], X_train[row][157], X_train[row][158],\n",
    "            X_train[row][-2], X_train[-1]), '| ',\n",
    "            '{:.0f}'.format(y_train[row][0])\n",
    "         )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define model\n",
    "\n",
    "### Network Structure\n",
    "\n",
    "經驗上：寬度 < 1000 個 neurons，深度 10 ~ 30 層，形狀用金字塔型\n",
    "\n",
    "深度可以用 sample/parameter raton 來估計，約 5 ~ 30\n",
    "\n",
    "1. A: 1 layer\n",
    "2. B: 2 layers, tower-shaped (一樣寬)\n",
    "3. C: 2 layers, pyramid-shaped (越來越窄)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A\n",
    "first_layer_width = 128\n",
    "second_layer_width = 0\n",
    "# B\n",
    "first_layer_width = 128\n",
    "second_layer_width = 128\n",
    "# C\n",
    "first_layer_width = 128\n",
    "second_layer_width = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "1. ReLU\n",
    "2. Sigmoid\n",
    "3. Tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_func = 'relu'\n",
    "activation_func = 'sigmoid'\n",
    "activation_func = 'tanh'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function\n",
    "1. Cross entropy\n",
    "2. Squared error (MSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = 'categorical_crossentropy'\n",
    "loss_func = 'mean_squared_error'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropout rate\n",
    "可以輸入想要的 dropout rate，例如 0%, 50%, 90% 等"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regularization\n",
    "1. No regularization: no\n",
    "2. L1-norm: l1\n",
    "3. L2-norm: l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-batch size\n",
    "可以設定 mini-batch 的大小，例如 128, 256, 512 等\n",
    "\n",
    "通常是 4 的倍數，這和硬體 cache 的設計有關 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Rate\n",
    "可以設定 learning rate 的大小，例如 0.1, 0.01, 0.5 等\n",
    "\n",
    "通常介於 0.01 到 0.1 之間\n",
    "\n",
    "同一個 epoch 的 learning rate 是常數"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 這邊才是定義 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DefineModel():\n",
    "    # Initialize\n",
    "    model = Sequential()\n",
    "    \n",
    "    # Add first hidden layer\n",
    "    model.add(Dense(first_layer_width, input_dim=784, W_regularizer=weight_regularizer))\n",
    "    model.add(Activation(activation_func))\n",
    "    if dropout_rate > 0:\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "    # Add second hidden layer\n",
    "    if second_layer_width > 0:\n",
    "        model.add(Dense(second_layer_width))\n",
    "        model.add(Activation(activation_func))\n",
    "        if droup_rate > 0:\n",
    "            model.add(Dropout(0.5))\n",
    "            \n",
    "    # Last layer has the same dimension as the number of classes\n",
    "    model.add(Dense(10))\n",
    "    # Then add softmax\n",
    "    model.add(Activation('softmax'))\n",
    "    # Define optimizer\n",
    "    opt = SGD(lr=learning_rate, clipnorm=5.)\n",
    "    # Define loss function\n",
    "    model.compile(loss=loss_function, optimizer=opt, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
