{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Yelp Dataset Challenge\n",
    "\n",
    "![Yelp Data Challenge](https://s3-media3.fl.yelpcdn.com/assets/srv0/engineering_pages/6d323fc75cb1/assets/img/dataset/960x225_dataset@2x.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('last_2_years_restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the values of the column that contains review text data, save to a variable named \"documents\"\n",
    "documents = df['text']\n",
    "\n",
    "# inspect your documents, e.g. check the size, take a peek at elements of the numpy array\n",
    "print(documents.shape)\n",
    "print('1:')\n",
    "print(documents[0])\n",
    "print('2:')\n",
    "print(documents[1])\n",
    "print('3:')\n",
    "print(documents[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define target\n",
    "\n",
    "* Target 要是 categorical variable\n",
    "* 可以考慮當 Target 的有 `avg_stars`, `cool`, `funny`, `stars`, `useful`\n",
    "  * 結果只有 `avg_stars` 和 `stars` 合適，其他幾個太多值了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the unique values\n",
    "for col in ['avg_stars', 'cool', 'funny', 'stars', 'useful']:\n",
    "    print(col + ':')\n",
    "    print(df[col].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 stars and non-5 stars rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a column and take the values, save to a variable named \"target\"\n",
    "df['five stars'] = (df['stars'] > 4)\n",
    "target = df['five stars']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Summary statistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.mean(), target.std(), target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and test splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(documents, target, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP representation\n",
    "\n",
    "1. Create `TfidfVectorizer`, and name it `vectorizer`\n",
    "2. Train the model with your training data\n",
    "3. Use the trained model to transform your test data\n",
    "4. Get the vocab of your tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words = 'english', max_features=5000)\n",
    "vec_train = vectorizer.fit_transform(X_train)\n",
    "vec_test = vectorizer.transform(X_test)\n",
    "vocab = vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train_array = vec_train.toarray()\n",
    "vec_test_array = vec_test.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(documents.shape[0] * 0.6, documents.shape[0] * 0.4)\n",
    "print(X_train.shape, X_test.shape)\n",
    "print(vec_train.shape, vec_test.shape, vec_train_array.shape, vec_test_array.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vec_train_array` and `vec_test_array` are sparse matrixes where a lot of elements are 0.\n",
    "It is hard to read from matrix form. So don't need to print and check them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vec_train_array) # sparse matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vec_test_array) # sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similar review search engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_top_values(lst, n, labels):\n",
    "    '''\n",
    "    Input: list, integer, list\n",
    "    Output: list\n",
    "    \n",
    "    Given a list of values, find the indices with the highest n values.\n",
    "    Return the labels for each of these indices\n",
    "    \n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = ['cat', 'dog', 'mouse', 'pig', 'rabbit']\n",
    "    output: ['cat', 'pig']\n",
    "    '''\n",
    "    # np.argsort() 預設是遞增排列，所以要用 -1 把排序反過來\n",
    "    return [labels[i] for i in np.argsort(lst)[::-1][:n]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bottom_values(lst, n, labels):\n",
    "    '''\n",
    "    Input: list, integer, list\n",
    "    Output: list\n",
    "    \n",
    "    Given a list of values, find the indices with the lowest n values.\n",
    "    Return the labels for each of thest indices.\n",
    "    \n",
    "    e.g.\n",
    "    lst = [7, 3, 2, 4, 1]\n",
    "    n = 2\n",
    "    labels = ['cat', 'dog', 'mouse', 'pig', 'rabbit']\n",
    "    output: ['mouse', 'rabbit']\n",
    "    '''\n",
    "    return [labels[i] for i in np.argsort(lst)[:n]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從 test sample 中隨便選一個 review，然後藉由 cosine similarity 來和 train samples 比較，選出 train samples 中前五名相似度最高的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Draw an arbitrary review from test (unseen in training) documents\n",
    "# random_number = np.random.randint(0, documents.shape[0]) # 只是用來驗證\n",
    "random_number = np.random.randint(0, vec_test.shape[0])\n",
    "# random_review = documents[random_number]\n",
    "random_review = X_test[random_number] # 用 test sample\n",
    "# print(vec_test.shape[0], X_test.shape)\n",
    "print(random_number)\n",
    "print(random_review)\n",
    "\n",
    "# Transform the drawn review(s) to vector(s)\n",
    "vec_random_review = vectorizer.transform([random_review])\n",
    "print(vec_random_review.shape)\n",
    "print(vec_random_review.toarray()) # 是 sparse 的 array\n",
    "\n",
    "# Calculate the similarity score(s) between vector(s) and training vectors\n",
    "similarity_scores = cosine_similarity(vec_random_review, vec_train) # vec_random_review: 1 x 5000, vec_train: 238822 x 5000\n",
    "print(similarity_scores.shape)\n",
    "print(similarity_scores) # vec_random_review 和 vec_train 中每一筆數據的 cosine similarity 值，vec_train 有 238822 筆數據，所以會有 238822 個數值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find top 5 similar reviews\n",
    "n = 5\n",
    "top_reviews = get_top_values(similarity_scores[0], n, X_train.values) # 和 train sample 中的來比較\n",
    "print(len(top_reviews))\n",
    "print(top_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print('Our search query:')\n",
    "# print(random_review) # To be added"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Most %s similar reviews:' % n)\n",
    "# print()  # To be added\n",
    "for index, review in enumerate(top_reviews):\n",
    "    print(str(index) + ':\\t' + review + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Does the result make sense to you?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying positive and negative review\n",
    "\n",
    "### Naive-Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "nb = MultinomialNB()\n",
    "nb.fit(vec_train, y_train)\n",
    "\n",
    "# Get score for training set and test set\n",
    "nb_score_train = nb.score(vec_train, y_train)\n",
    "nb_score_test = nb.score(vec_test, y_test)\n",
    "\n",
    "print('train score: ', nb_score_train)\n",
    "print('test score: ', nb_score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "lr.fit(vec_train, y_train)\n",
    "\n",
    "# Get score for training set and test set\n",
    "lr_score_train = lr.score(vec_train, y_train)\n",
    "lr_score_test = lr.score(vec_test, y_test)\n",
    "\n",
    "print('train score: ', lr_score_train)\n",
    "print('test score: ', lr_score_test)\n",
    "\n",
    "print(lr.coef_.shape)\n",
    "print(lr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What are the key features (words) that make the positive prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find it out by ranking\n",
    "n = 20\n",
    "features_with_positive_predictions = get_top_values(lr.coef_[0], n, vocab)\n",
    "print(features_with_positive_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What are the key features(words) that make the negative prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's find it out by ranking\n",
    "n = 20\n",
    "features_with_negative_predictions = get_bottom_values(lr.coef_[0], n, vocab)\n",
    "print(features_with_negative_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(max_depth = None, n_estimators = 5, min_samples_leaf = 10)\n",
    "rf.fit(vec_train, y_train)\n",
    "\n",
    "# Get score for training set and test set\n",
    "rf_score_train = rf.score(vec_train, y_train)\n",
    "rf_score_test = rf.score(vec_test, y_test)\n",
    "\n",
    "print('train score: ', rf_score_train)\n",
    "print('test score: ', rf_score_test)\n",
    "\n",
    "print(rf.feature_importances_.shape)\n",
    "print(rf.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: What do you see from the training score and the test score?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: train score is higher than test score, it means overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q: Can you tell what features (words) are important by inspecting the RFC model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 20\n",
    "top_features_importances = get_top_values(rf.feature_importances_, n, vocab)\n",
    "print(top_features_importances)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the top 10 words in Logistic Regression and Random Forest models\n",
    "\n",
    "|words     |Logistic Regression|Random Forest|\n",
    "|:--------:|:-----------------:|:-----------:|\n",
    "|amazing   |1                  |1            |\n",
    "|best      |2                  |3            |\n",
    "|incredible|3                  |             |\n",
    "|thank     |4                  |             |\n",
    "|awesome   |5                  |             |\n",
    "|delicious |6                  |7            |\n",
    "|perfection|7                  |             |\n",
    "|highly    |8                  |             |\n",
    "|phenomenal|9                  |             |\n",
    "|perfect   |10                 |             |\n",
    "|didn      |x                  |2            |\n",
    "|great     |x                  |4            |\n",
    "|worst     |x                  |5            |\n",
    "|love      |x                  |6            |\n",
    "|bad       |x                  |8            |\n",
    "|said      |x                  |9            |\n",
    "|definitely|x                  |10           |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Credit\n",
    "\n",
    "### 1. Use cross validation to evaluate classifiers\n",
    "\n",
    "[sklearn cross validation](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(lr, vec_train, y_train, cv=5, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Use grid search to find best predictable classifier\n",
    "\n",
    "[sklearn grid search tutorial (with cross validation)](https://scikit-learn.org/stable/modules/grid_search.html#grid-search)\n",
    "\n",
    "[sklearn grid search documentation (with cross validation)](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV)\n",
    "\n",
    "[Model evaluation](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter)\n",
    "\n",
    "[classification_report](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "params = {'penalty':['l1', 'l2'], 'C':[0.1, 1, 10, 100]}\n",
    "scorings = ['accuracy']\n",
    "\n",
    "for score in scorings:\n",
    "    print(\"Tuning hyper-parameters for {}\".format(score))\n",
    "    clf = GridSearchCV(lr, params, cv=5, scoring=score)\n",
    "    clf.fit(vec_train, y_train)\n",
    "    \n",
    "    print(\"Best parameters set found on development set:\\n\")\n",
    "    print(clf.best_params_)\n",
    "    print('\\n')\n",
    "    \n",
    "    cv_result = clf.cv_results_\n",
    "    means_train = cv_result['mean_train_score']\n",
    "    stds_train = cv_result['std_train_score']\n",
    "    means_test = cv_result['mean_test_score']\n",
    "    stds_test = cv_result['std_test_score']\n",
    "    \n",
    "    for params, mean_train, std_train, mean_test, std_test in zip(cv_result['params'], means_train, stds_train, means_test, stds_test):\n",
    "        print('For params:', params)\n",
    "        print('Train: {:.3f} +/- {:.3f}'.format(mean_train, std_train))\n",
    "        print('Test: {:.3f} +/- {:.3f}'.format(mean_test, std_test))\n",
    "    \n",
    "\n",
    "    print('Classification report:')\n",
    "    print('The model is trained on the full development set.\\n\\\n",
    "           The scores are computed on the full evaluation set.')\n",
    "    y_pred = clf.predict(vec_test)\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
